int kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0){
 unsigned long old_cr0 = kvm_read_cr0(vcpu);
 unsigned long update_bits = X86_CR0_PG | X86_CR0_WP |
        X86_CR0_CD | X86_CR0_NW;

 cr0 |= X86_CR0_ET;

#ifdef CONFIG_X86_64
 if (cr0 & 0xffffffff00000000UL)
  return 1;
#endif

 cr0 &= ~CR0_RESERVED_BITS;

 if ((cr0 & X86_CR0_NW) && !(cr0 & X86_CR0_CD))
  return 1;

 if ((cr0 & X86_CR0_PG) && !(cr0 & X86_CR0_PE))
  return 1;

 if (!is_paging(vcpu) && (cr0 & X86_CR0_PG)){
#ifdef CONFIG_X86_64
  if ((vcpu->arch.efer & EFER_LME)){
   int cs_db, cs_l;

   if (!is_pae(vcpu))
    return 1;
   kvm_x86_ops->get_cs_db_l_bits(vcpu, &cs_db, &cs_l);
   if (cs_l)
    return 1;
  } else
#endif
  if (is_pae(vcpu) && !load_pdptrs(vcpu, vcpu->arch.walk_mmu,
       kvm_read_cr3(vcpu)))
   return 1;
 }

 if (!(cr0 & X86_CR0_PG) && kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE))
  return 1;

 kvm_x86_ops->set_cr0(vcpu, cr0);

 if ((cr0 ^ old_cr0) & X86_CR0_PG){
  kvm_clear_async_pf_completion_queue(vcpu);
  kvm_async_pf_hash_reset(vcpu);
 }

 if ((cr0 ^ old_cr0) & update_bits)
  kvm_mmu_reset_context(vcpu);
 return 0;
}
